# Q-A-using-ollama
This project is a fully offline, multi-file Question Answering system built with Streamlit, LangChain, FAISS, and Ollama. It allows users to upload multiple PDF, DOCX, or text files via a drag-and-drop interface, extract and split their content into manageable chunks, and embed them using TF-IDF embeddings. The system uses FAISS for efficient document retrieval and a local Ollama language model (like Mistral, TinyLlama) to generate accurate answers based on the most relevant document contexts â€” all without needing internet access.

